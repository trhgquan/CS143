#!usr/bin/julia

using CSV
using DataFrames
using Random
using Statistics

# For testing only
#=
using DecisionTree
=#

Tree = Nothing
depth = 0

# Split dataframe into parts with percentage.
# Kudos to Bogumił Kamiński - https://stackoverflow.com/a/66059719
function splitdf(df, pct)
  @assert 0 <= pct <= 1

  ids = collect(axes(df, 1))

  shuffle!(ids)

  sel = ids .<= nrow(df) .* pct

  return view(df, sel, :), view(df, .!sel, :)
end

# Calculate Entropy
function entropy(counts, n_samples)
  prob = counts / float.(n_samples)
  return -sum(prob .* log.(prob))
end

function entropy_of_one_division(division)
  n_samples = length(division)
  n_classes = Set(division)

  counts = []
  for s in n_classes
    temp_count = 0
    
    for i in 1:n_samples
      if division[i] == s
        temp_count += 1
      end
    end

    push!(counts, temp_count)
  end

  return entropy(counts, n_samples), n_samples
end

function get_entropy(y_predict, y)
  n = length(y)

  left = y_predict ? 1 : 0
  right = y_predict ? 0 : 1

  entropy_true, n_true = entropy_of_one_division(y[~y_predict])
  entropy_false, n_false = entropy_of_one_division(y[y_predict])

  s = entropy_true - entropy_false

  return s
end

function fit!(X, y, node = Dict(), depth = 0)
  if all(y == y[1])
    return Dict("val" => y[1])
  end

  col_idx, cutoff, entropy = find_best_split_of_all(X, y)
  y_left = y[X[:, col_idx] < cutoff]
  y_right = y[X[:, col_idx] >= cutoff]
    
  node["index_col"] = col_idx
  node["cutoff"] = cutoff
  node["val"] = mean(y)
  node["left"] = fit(X[X[:, col_idx] < cutoff], y_left, Dict(), depth + 1)
  node["right"] = fit(X[X[:, col_idx] >= cutoff], y_right, Dict(), depth + 1)

  depth += 1
  Tree = node

  return node
end

function find_best_split_of_all(X, y)
  col_idx, cutoff = Nothing, Nothing
  min_entropy = 1

  for (i, col_data) in enumerate(X)
    entropy, cur_cutoff = find_best_split(col_data, y)

    if entropy == 0
      return i, cur_cutoff, entropy
    end

    if entropy <= min_entropy
      min_entropy = entropy
      col_idx = i
      cutoff = cur_cutoff
    end
  end

  return col_idx, cutoff, entropy
end

function find_best_split(col_data, y)
  min_entropy = 10
  cutoff = Nothing

  for value in Set(col_data)
    y_predict = col_data < value
    my_entropy = get_entropy(y_predict, y)

    if my_entropy <= min_entropy
      min_entropy = my_entropy
      cutoff = value
    end

    return min_entropy, cutoff
  end
end

function predict(X)
  tree = Tree
  pred = zeros(length(X))

  for (i, c) in enumerate(X)
    pred[i] = _predict(c)
  end

  return pred
end

function _pred(row)
  cur_layer = Tree
    
  early_stop = get(cur_layer, "cutoff", false)

  if early_stop == false
    return cur_layer["val"]
  end

  while get(cur_layer, "cutoff", false)
    if row[cur_layer["index_col"]] < cur_layer["cutoff"]
      cur_layer = cur_layer["left"]
    else
      cur_layer = cur_layer["right"]
    end
  end
end

# Loading dataset
raw_df = DataFrame(CSV.File("iris.csv"))

# Split dataset to training (2/3) and testing (1/3)
train_df, test_df = splitdf(raw_df, 0.666666667)

X_train, X_test = train_df[!, Not("variety")], test_df[!, Not("variety")]
y_train, y_test = train_df[!, "variety"], test_df[!, "variety"]

X_train, X_test = float.(Matrix(X_train)), float.(Matrix(X_test))
y_train, y_test = string.(y_train), string.(y_test)

tree = fit!(X_train, y_train)
train_pred = predict(X_train)
println("Accuracy of decision tree on training data: $(train_pred)")
test_pred = predict(X_test)
println("Accuracy of decision tree on testing data: $(test_pred)")

#=
# For testing only.
model = DecisionTreeClassifier(max_depth = 2)
fit!(model, X_train, y_train)

print_tree(model, 5)

using ScikitLearn.CrossValidation: cross_val_score

accuracy = cross_val_score(model, X_test, y_test, cv = 3)
=#
